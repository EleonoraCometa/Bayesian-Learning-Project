---
title: "IPI Bayesian Regression"
output: html_document
date: "2024-08-10"
---

# A Bayesian Approach to Modeling the Industrial Production Index

## Overview

This notebook contains the R code for the Bayesian analysis of the Industrial Production Index (IPI). The detailed explanation of the results are exposed in the report. The modelling is performed using the `BAS` library.

### Package and Data import Package import:

```{r}
library("ggplot2")
library("BAS")
library("loo")
library(tidyr)
```

Read data:

```{r}
data_complete = read.csv("indprod.csv")
data_complete$DATE = as.Date(data_complete$DATE)

```

## Data Pre-Processing

First visualization of the dataset:

```{r}
g1 = ggplot(data_complete, aes(x = DATE, y = INDPRO_PCH)) +
  geom_point(color = "darkgrey", size = 2) +  # Punti neri
  labs(title = "Time-Series of IPI", x = "Date", y = "IPI Index") +  
  theme_minimal() + 
  theme(
    plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
    axis.title.x = element_text(size = 14),  
    axis.title.y = element_text(size = 14),  
    axis.text = element_text(size = 12)  
  )   
g1
```

The dataset contains a lot of outliers that can increase the model's error. For this reason we can filter the data where the modulus is to far with respect to the others.

See: [Bayesian Robustness to Outliers in Linear Regression and Ratio Estimation](https://arxiv.org/pdf/1612.05307).

```{r}
data_limit = 3
g2 = g1 +
  geom_point(data = subset(data_complete, INDPRO_PCH > data_limit | INDPRO_PCH < -data_limit), 
             aes(x = DATE, y = INDPRO_PCH), 
             color = "red", 
             size = 4.0, shape = 21, stroke = 1.0) + 
  theme_minimal()
g2
```

Deleting outliers from dataset:

```{r}
limit_min = -3
limit_max = 3
data = subset(data_complete, INDPRO_PCH >= limit_min & INDPRO_PCH <= limit_max) 
```

```{r}
g3 = ggplot(data, aes(x = DATE, y = INDPRO_PCH)) +
    geom_point(color = "darkgrey", size = 2) +
    labs(title = "Time-Series of IPI", x = "Date", y = "IPI Index") +  
    theme_minimal() + 
    theme(
      plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
      axis.title.x = element_text(size = 14),  
      axis.title.y = element_text(size = 14),  
      axis.text = element_text(size = 12)  
    )   
g3
```

```{r}
# Define covarietes
data_ = data[, -1]
```

We split randomly the dataset in two parts: - `train_data`: contains the data used for fitting the model - `test_data` : contains the data used for test the model

```{r}
set.seed(123) 
train_index = sample(1:nrow(data_), 0.7 * nrow(data_))

# Training set
train_data  = data[train_index, ]
train_data_ = data_[train_index, ] 

# Testing set
test_data  = data[-train_index, ]
test_data_ = data_[-train_index, ]
```

# Bayesian Linear Regression

We try to fit the model with different priors: - Zellner's G-prior - Zellner's JZS For a better readability of the code we define a function based on `bas.lm`.

```{r}
bas_lm = function (data= train_data_, prior="g-prior", alpha=1.0){
  local_model = bas.lm(INDPRO_PCH ~ ., data =  data, 
                       prior=prior, alpha=alpha, 
                       modelprior = Bernoulli(1), 
                       include.always = ~ ., 
                       n.models = 1)
  return(local_model)
}
```

A useful function for plotting credible intervals:

```{r}
plot_credible_interval = function(intervals, alpha){
  # Conversion to dataframe for better plotting
  df = as.data.frame(intervals)[1:12, ]
  ggplot(df, aes(x = rownames(df), y = df[, 3], ymin = df[,1], ymax = df[,2])) +
    geom_point(size = 0.1) +
    geom_pointrange() +
    labs(title = paste("  Credible intervals with alpha = ", alpha),
         y ="Estimate", x = NULL) +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
}
```

```{r}
compare_different_alpha = function(data_train, data_test, prior, alphas){
  
  results = data.frame(DATE=data_test$DATE)
  for (alpha in alphas) {
  
    m = bas_lm(data=data_train, prior=prior, alpha=alpha)
    
    pred = predict(m, newdata=data_test)
    
    results[[paste("alpha=", alpha, sep = "")]] = pred$fit
  }
  return(results)
}
```

## Zellner's G-prior

```{r}
alphapar = 10.0
model_g_prior = bas_lm(data=train_data_, "g-prior", alphapar)
beta_g_prior = coef(model_g_prior)
beta_g_prior
```

```{r}
plot(beta_g_prior, subset = 1:12, ask=F)
```

```{r}
ci_g_prior = confint(beta_g_prior)
plot_credible_interval(ci_g_prior, alphapar)
```

Manual definition of the model using different value of the parameters: in particular we use the posterior mean, the upper and lower bounds of the credible interval.

```{r}
data_1 = data_[, -1]
beta_mean = beta_g_prior$postmean

# Model 
y_hat =  beta_mean[1] + 
         beta_mean[2] *   train_data_$WILL5000PR_PCH + 
         beta_mean[3] *   train_data_$HSN1F_PCH + 
         beta_mean[4] *   train_data_$DCOILBRENTEU_PCH + 
         beta_mean[5] *   train_data_$CORESTICKM159SFRBATL_PCH + 
         beta_mean[6] *   train_data_$TOTALSA_PCH + 
         beta_mean[7] *   train_data_$CPIUFDSL_PCH + 
         beta_mean[8] *   train_data_$DEXJPUS_PCH + 
         beta_mean[9] *   train_data_$MICH_PCH + 
         beta_mean[10] *  train_data_$VIXCLS_PCH + 
         beta_mean[11] *  train_data_$PAYEMS_PCH + 
         beta_mean[12] *  train_data_$PPIACO_PCH
```

```{r}
out = confint(beta_g_prior)[, 1:2]
beta_low = out[,1]
beta_up = out[,2]

y_low =  beta_low[1] + 
         beta_low[2] *   train_data_$WILL5000PR_PCH + 
         beta_low[3] *   train_data_$HSN1F_PCH + 
         beta_low[4] *   train_data_$DCOILBRENTEU_PCH + 
         beta_low[5] *   train_data_$CORESTICKM159SFRBATL_PCH + 
         beta_low[6] *   train_data_$TOTALSA_PCH + 
         beta_low[7] *   train_data_$CPIUFDSL_PCH + 
         beta_low[8] *   train_data_$DEXJPUS_PCH + 
         beta_low[9] *   train_data_$MICH_PCH + 
         beta_low[10] *  train_data_$VIXCLS_PCH + 
         beta_low[11] *  train_data_$PAYEMS_PCH + 
         beta_low[12] *  train_data_$PPIACO_PCH

y_up =   beta_up[1] + 
         beta_up[2] *   train_data_$WILL5000PR_PCH + 
         beta_up[3] *   train_data_$HSN1F_PCH + 
         beta_up[4] *   train_data_$DCOILBRENTEU_PCH + 
         beta_up[5] *   train_data_$CORESTICKM159SFRBATL_PCH + 
         beta_up[6] *   train_data_$TOTALSA_PCH + 
         beta_up[7] *   train_data_$CPIUFDSL_PCH + 
         beta_up[8] *   train_data_$DEXJPUS_PCH + 
         beta_up[9] *   train_data_$MICH_PCH + 
         beta_up[10] *  train_data_$VIXCLS_PCH + 
         beta_up[11] *  train_data_$PAYEMS_PCH + 
         beta_up[12] *  train_data_$PPIACO_PCH
```

```{r}
# Plot
plot1 = 
  ggplot(data = train_data, aes(x = DATE, y = INDPRO_PCH)) + 
  geom_point(color = "steelblue", alpha=0.5) + xlab("DATE") +
  geom_line(data = train_data, 
            aes(x = DATE, y = y_hat, color = "first"),  lty = 1) +
  geom_line(data = train_data, 
            aes(x = DATE, y = y_low, color = "second", lty = "second")) +
  geom_line(data = train_data, 
            aes(x = DATE, y = y_up,  color= "second", lty = "third")) +
  scale_colour_manual(values=c("first"="darkorange", "second"="darkgrey"), labels=c("Posterior Mean","95% C.I."), name="") +
  scale_linetype_manual(values = c(1, 2), labels = c("lower" , "upper"),
                        name = "95% C.I.")

plot1

y_fit = predict(model_g_prior, newdata = train_data_)$fit
plot2 = 
  ggplot(data = train_data, aes(x = DATE, y = INDPRO_PCH)) + 
  geom_point(color = "steelblue", alpha=0.5) + xlab("DATE") +
  geom_line(data = train_data, 
            aes(x = DATE, y = y_fit, color = "first"),  lty = 1) +
  scale_colour_manual(values="darkorange", labels="Posterior Mean", name="") +
  scale_linetype_manual(values = c(1, 2), labels = c("lower" , "upper"),
                        name = "95% C.I.")

plot2
```

Predicitons using the test dataset and the posterior mean of vector beta

```{r}
predictions = predict(model_g_prior, 
                      newdata = test_data_,
                      se.fit = TRUE)
pred_intervals = confint(predictions, level = 0.95)
y_pred_low = pred_intervals[,1]
y_pred_up = pred_intervals[,2]

ggplot(data = test_data, aes(x = DATE, y = INDPRO_PCH)) + 
  geom_point(color = "steelblue", alpha=0.5) + xlab("DATE") +
  geom_line(data = test_data, 
            aes(x = DATE, y = predictions$fit, color = "first"), lty = 1) +
  geom_line(data = test_data, 
            aes(x = DATE, y = y_pred_low, color = "second", lty = "second")) +
  geom_line(data = test_data, 
            aes(x = DATE, y = y_pred_up, color = "second", lty = "third")) +
  
  scale_colour_manual(values = c("first" = "darkorange", "second" = "darkgrey"), 
                      labels = c("Posterior Mean", "95% C.I."), 
                      name = "") +
  scale_linetype_manual(values = c(1, 2, 2),
                        labels = c("Lower", "Upper"),
                        name = "95% C.I.") +
  labs(x="DATE", y="IPI")
```

### Model Selection using Bayesian Information Criterion (BIC)

```{r}
cog.BIC = bas.lm(INDPRO_PCH ~ ., data = train_data_,
                 prior = "BIC", modelprior = uniform())

round(summary(cog.BIC), 2)

# Find the index of the model with largest `logmarg`:
cog.best = which.max(cog.BIC$logmarg)

# Print the selected subset of variables:
selectedVars = cog.BIC$which[[cog.best]]+1
cog.BIC$namesx[selectedVars]

```

Now we will proceed with the same steps using only the variables selected with BIC.

```{r}
vars = cog.BIC$namesx[selectedVars]
vars = c("DATE", "INDPRO_PCH", vars)
vars = intersect(vars, names(train_data))  # Filtra solo le colonne esistenti

ms_data_train = train_data[, vars]
ms_data_train_ = ms_data_train[-1]

ms_data_test = test_data[, vars]
ms_data_test_ = ms_data_test[-1]
```

We build and train the model using the g-prior.

```{r}
alphapar = 10
model_g_prior_ms = bas_lm(data=ms_data_train_, prior="g-prior", alpha=alphapar)
```

```{r}
beta_ms = coef(model_g_prior_ms)
beta_ms
```

```{r}
plot(beta_ms, ask=F)
```

```{r}
predictions_ms = predict(model_g_prior_ms, newdata = ms_data_test_, se.fit = TRUE)

ggplot(data = test_data, aes(x = DATE, y = INDPRO_PCH)) + 
geom_point(color = "steelblue", alpha=0.5) + xlab("DATE") +
geom_line(data = ms_data_test, aes(x = DATE, y = predictions_ms$fit, color = "With model selection"),  lty = 1) +
geom_line(data = test_data, aes(x = DATE, y = predictions$fit, color="Complete model"), lty=1) +
labs(
  x="Date",
  y="Prediction"
)+
scale_color_manual(name = "Model", values = c("With model selection" = "blue", "Complete model" = "darkorange"))
  

```

Now, the idea is analyze the behaviour of the model when alpha changes.

```{r}
alphas = c(0.001, 0.1, 1, 10, 100, dim(ms_data_train_)[1])
results = compare_different_alpha(ms_data_train_, ms_data_test, "g-prior", alphas)
results
```

We plot the different predictions in a single image:

```{r}
# Change the dataframe into long
results_long = results %>%
  pivot_longer(cols = -DATE, 
               names_to = "alpha", 
               values_to = "predictions")

ggplot(results_long, aes(x = DATE, y = predictions, color = alpha)) +
  geom_line() +
  geom_point(data = ms_data_test, 
             aes(x = DATE, y = INDPRO_PCH), 
             color = "steelblue", 
             shape = 16, 
             alpha=0.5) +
  labs(x = "Date", 
       y = "Predictions") 

```

Here we compute the Mean Square Error (MSE) for each model in order to compere different models.

```{r}
mse = function(actual, predicted) {
  mean((actual - predicted)^2)
}

mse_values = numeric(length(alphas))

# Compute MSE for each value of alpha
for (i in 1:length(alphas)) {
  alpha_column = paste("alpha", alphas[i], sep = "=")
  mse_values[i] = mse(ms_data_test$INDPRO_PCH, results[[alpha_column]])
}
mse_df = data.frame(alpha = alphas, MSE = mse_values)
mse_df
```

## Zellner's JZS

```{r}
alphapar = 1
model_jzs_prior = bas_lm(data=train_data, prior="JZS", alpha=alphapar)
beta_jzs = coef(model_jzs_prior)
beta_jzs
```

```{r}
plot(beta_jzs, subset = 1:12, ask = F)
```

```{r}
credible_intervals_jzs= confint(beta_jzs)
plot_credible_interval(credible_intervals_jzs, alpha = alphapar)
```

```{r}
BF_2 <- exp(model_g_prior$logmarg - model_jzs_prior$logmarg)
```


```{r}
results_alpha = compare_different_alpha(ms_data_train, 
                                        ms_data_test, 
                                        prior="JZS",
                                        c(0.1, 1, 10, 100, dim(ms_data_train)[1]))
# Change the dataframe into long
results_long = results_alpha %>%
  pivot_longer(cols = -DATE, 
               names_to = "alpha", 
               values_to = "predictions")

ggplot(results_long, aes(x = DATE, y = predictions, color = alpha)) +
  geom_line() +
  geom_point(data = ms_data_test, 
             aes(x = DATE, y = INDPRO_PCH), 
             color = "steelblue", 
             shape = 16, 
             alpha=0.5) +
  labs(x = "Date", 
       y = "Predictions") 
```

## Plot actual values against predicted ones using Test Set

```{r}
# Compute the predictions:
pred.model_g_prior <- predict(model_g_prior, test_data_, estimator="BMA")

# Create the actual and predicted variables:
actualValues = test_data_$INDPRO_PCH
predictedValues = pred.model_g_prior$fit

# Compute the residuals:
residValues = predictedValues - actualValues

# Plot actual vs predicted values:
Pl1 = ggplot() + geom_point(aes(x = actualValues, y = predictedValues), color = "steelblue") +
  xlab("Actual") + ylab("Predicted") + ggtitle("Predictions of INDPRO_CPH")

# Plot the residuals:
Pl2 = ggplot() + geom_point(aes(x = predictedValues, y = residValues), color = "darkred") +
  xlab("Predicted") + ylab("Residuals") + ggtitle("Predicted VS Residuals")

library(gridExtra)
grid.arrange(Pl1, Pl2, ncol=2)
```

```{r}
# Compute the predictions:
pred.model_g_prior_2 <- predict(model_g_prior_2, test_data_, estimator="BMA")

# Create the actual and predicted variables:
actualValues = test_data_$INDPRO_PCH
predictedValues = pred.model_g_prior_2$fit

# Compute the residuals:
residValues = predictedValues - actualValues

# Plot actual vs predicted values:
Pl1 = ggplot() + geom_point(aes(x = actualValues, y = predictedValues), color = "steelblue") +
  xlab("Actual") + ylab("Predicted") + ggtitle("Predictions of INDPRO_CPH")

# Plot the residuals:
Pl2 = ggplot() + geom_point(aes(x = predictedValues, y = residValues), color = "darkred") +
  xlab("Predicted") + ylab("Residuals") + ggtitle("Predicted VS Residuals")

library(gridExtra)
grid.arrange(Pl1, Pl2, ncol=2)
```
